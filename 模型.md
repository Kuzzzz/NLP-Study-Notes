#模型笔记
##Transformer
encoder-decoder架构，分别有6层encoder和6层decoder  
定义输入序列经过word embedding，和position embedding相加后，输入到encoder中，输出序列经过的处理和输入序列一样，之后输入到decoder中。  
![Transformer](/users/anlei/desktop/笔记/pic/Transformer.png)  
###Encoder
• multi-head self-attention  
• position-wise feed-forward network  
每个部分后面都接了一个残差连接和Layer Normalization  
###Decoder
• multi-head self-attention  
• multi-head context-attention  
• position-wise feed-forward network  
每个部分后面都接了一个残差连接和Layer Normalization  

##Fasttext
核心思想：**将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。这中间涉及到两个技巧：字符级n-gram特征的引入以及分层Softmax分类**  

##Word2vec
w2v训练时的参数有：词向量维度、上下文距离（窗口大小）、cbow/skipgram、2个训练加速方法、需要计算词向量的最小词频、SGD迭代最大次数、初始步长  
**word2vec分为CBOW和Skip-gram两种模型以及Hierarchical Softmax和Negative Sampling两种改进方法。**  
最开始的神经网络训练词向量是由输入层+隐藏层+输出层(Softmax)组成的。  CBOW模型的输入是上下文n个词的词向量，输出是所有词的softmax概率，找到概率最大的词对应的神经元作为预测词。  
Skip-gram与CBOW相反，输入是一个特定的词的词向量，输出是特定词对应的上下文词向量（Softmax概率排前N的n个词）  
word2vec在DNN模型上进行了优化，因为DNN在计算时十分耗时，所以想到采用哈夫曼树来代替隐藏层和输出层的神经元（叶子节点=输出层神经元，内部节点=隐藏层神经元，叶子节点个数==词汇表大小）  
对叶子节点进行哈夫曼编码，权重高的叶子节点越靠近根节点，所以树的带权路径最短，因为是二叉树所以计算量从V变成logV（w2v中左子树编码为1，右为0，左权重不小于右权重）
###基于Hierarchical Softmax的模型
在哈弗曼树中，隐藏层到输出层的softmax映射是沿着哈弗曼树一步步完成的，所以叫hierarchical softmax。  
采用二元逻辑回归，规定沿着左子树走就是负类，沿着右子树走就是正类，判别正负类的方法是sigmoid函数：P(+)=\sigma\left(x_{w}^{T} \theta\right)=\frac{1}{1+e^{-x_{w}^{T} \theta}}  x_{w}是当前内部结点的词向量，\theta是需要从训练样本中求出的LR的参数。  
判断是沿左子树还是右子树的标准是看P(+)和P(-)谁的概率值大，控制谁大的因素是xw和当前节点的模型参数\theta  
基于Hierarchical Softmax的word2vec本身，我们的目标就是找到合适的所有节点的词向量和所有内部节点θ, 使训练样本达到最大似然。  
![hierarchical softmax](/users/anlei/desktop/笔记/pic/hierarchical_softmax.png)  
##LSTM
![LSTM](/users/anlei/desktop/笔记/pic/lstm.png)  
LSTM有三个门，分别是遗忘门、输入门、输出门。  
遗忘门确定前一个步长中哪些相关的信息需要被保留；  
输入门确定当前输入中哪些信息是重要的，需要被添加的；  
输出门确定下一个隐藏状态应该是什么  
cell 最上面的一条线的状态即 s(t) 代表了长时记忆，而下面的 h(t)则代表了工作记忆或短时记忆
###遗忘门
遗忘门的功能是决定应丢弃或保留哪些信息。来自前一个隐藏状态的信息和当前输入的信息同时传递到 sigmoid 函数中去，输出值介于 0 和 1 之间，越接近 0 意味着越应该丢弃，越接近 1 意味着越应该保留。
###输入门
输入门用于更新细胞状态。首先将前一层隐藏状态的信息和当前输入的信息传递到 sigmoid 函数中去。将值调整到 0~1 之间来决定要更新哪些信息。0 表示不重要，1 表示重要。  
其次还要将前一层隐藏状态的信息和当前输入的信息传递到 tanh 函数中去，创造一个新的侯选值向量。最后将 sigmoid 的输出值与 tanh 的输出值相乘，sigmoid 的输出值将决定 tanh 的输出值中哪些信息是重要且需要保留下来的。
###细胞状态
下一步，就是计算细胞状态。首先前一层的细胞状态与遗忘向量逐点相乘。如果它乘以接近 0 的值，意味着在新的细胞状态中，这些信息是需要丢弃掉的。然后再将该值与输入门的输出值逐点相加，将神经网络发现的新信息更新到细胞状态中去。至此，就得到了更新后的细胞状态。
###输出门
输出门用来确定下一个隐藏状态的值，隐藏状态包含了先前输入的信息。首先，我们将前一个隐藏状态和当前输入传递到 sigmoid 函数中，然后将新得到的细胞状态传递给 tanh 函数。  
最后将 tanh 的输出与 sigmoid 的输出相乘，以确定隐藏状态应携带的信息。再将隐藏状态作为当前细胞的输出，把新的细胞状态和新的隐藏状态传递到下一个时间步长中去。
###参数计算方法
在一个cell中，假设num_units=m，输入维度为n，由于有4个全连接层（3个sigmoid，1个tanh）所以参数数量为：4*((m+n) *m+m)  
解释：有4个全连接层，所以最外层为4;m+n为输入与ht-1拼接后的维度；m+n作为4个全连接层的输入，由于num_units=m，m个神经元与m+n的输入全部连接，所以是(m+n)*m；最后一个m是因为有m个bias  
##TextRank
利用TextRank做文本自动摘要
将文本中的每个句子分别看做一个节点，如果两个句子有相似性，那么认为这两个句子对应的节点之间存在一条无向有权边。考察句子相似度的方法是下面这个公式：
![textrank](/users/anlei/desktop/笔记/pic/textrank.png) 
公式中，Si,Sj分别表示两个句子词的个数总数，Wk表示句子中的词，那么分子部分的意思是同时出现在两个句子中的同一个词的个数，分母是对句子中词的个数求对数之和。分母这样设计可以遏制较长的句子在相似度计算上的优势。

我们可以根据以上相似度公式循环计算任意两个节点之间的相似度，根据阈值去掉两个节点之间相似度较低的边连接，构建出节点连接图，然后计算TextRank值，最后对所有TextRank值排序，选出TextRank值最高的几个节点对应的句子作为摘要。  
textrank值的计算方法：
![textrank2](/users/anlei/desktop/笔记/pic/textrank2.png)
d为平滑项，ln(Vi)为节点Vi的前驱节点的集合，Out(Vi)为Vi后继节点的集合，wji表示两个节点之间的边连接有不同的重要程度
##HMM
HMM是生成模型：在已知要标注的句子s的情况下，去判断生成标注序列l的概率，如下所示：
![hmm_p](/users/anlei/desktop/笔记/pic/hmm_p.png)
p(l_i|l_i-1)是转移概率，比如，l_i-1是介词，l_i是名词，此时的p表示介词后面的词是名词的概率。
p(w_i|l_i)表示发射概率（emission probability），比如l_i是名词，w_i是单词“ball”，此时的p表示在是名词的状态下，是单词“ball”的概率。

##CRF
设X、Y是随机变量，P(Y|X)是给定X时Y的条件概率，若随机变量Y构成的是一个马尔科夫随机场，则称条件概率P(Y|X)是条件随机场。  
###CRF中的特征函数
>例如序列标注问题：有5个单词，我们将：(名词，动词，名词，介词，名词)作为一个标注序列，称为l，可选的标注序列有很多种，比如l还可以是这样：（名词，动词，动词，介词，名词），我们要在这么多的可选标注序列中，挑选出一个最靠谱的作为我们对这句话的标注。假如我们给每一个标注序列打分，打分越高代表这个标注序列越靠谱，我们至少可以说，凡是标注中出现了动词后面还是动词的标注序列，要给它负分！！  
上面所说的动词后面还是动词就是一个特征函数，我们可以定义一个特征函数集合，用这个特征函数集合来为一个标注序列打分，并据此选出最靠谱的标注序列。也就是说，每一个特征函数都可以用来为一个标注序列评分，把集合中所有特征函数对同一个标注序列的评分综合起来，就是这个标注序列最终的评分值。  

CRF的特征函数接收4个参数：

* 句子s
* i，用来表示句子中的第i个单词
* l_i表示要评分的标注序列给第i个单词标注的词性
* l_i-1表示要评分的标注序列给第i-1个单词标注的词性

它的输出值是0或者1,0表示要评分的标注序列不符合这个特征，1表示要评分的标注序列符合这个特征。（我们的特征函数仅仅依靠当前单词的标签和它前面的单词的标签对标注序列进行评判，这样建立的CRF也叫作线性链CRF）  
###从特征函数到概率
给每个特征函数f_j赋一个权重λ_j，给定一个句子s，有一个标注序列l，可以利用下述公式定义的特征函数集来评分：
![crf_score](/users/anlei/desktop/笔记/pic/crf_score.png)
外面的求和用来求每一个特征函数f_j评分值的和，里面的求和用来求句子中每个位置的单词的的特征值的和。  
对这个分数进行指数化和标准化，我们就可以得到标注序列l的概率值p(l|s)，如下所示：
![crf_p](/users/anlei/desktop/笔记/pic/crf_P.png)
##TextCNN
pass
##GCN
假设有图中有N个节点，这些节点的特征组成一个NxD维的矩阵X，各个节点之间的关系组成一个NxN维的矩阵A，X和A是模型的输入。  
GCN的层与层之间的传播方式为：  
![GCN](/users/anlei/desktop/笔记/pic/GCN.png)  

* A波浪=A+I，I是单位矩阵  
* D波浪是A波浪的度矩阵（degree matrix），![GCN_D](/users/anlei/desktop/笔记/pic/GCN_D.png)
* H是每一层的特征，对于输入层的话，H就是X
* σ是非线性激活函数  
**其中D^(-1/2)AD^(1/2)是可以事先算好的，因为D波浪是由A计算而来，而A是输入之一。**  
GCN输入一个图，通过若干层后GCN每个node的特征从X变成了Z，但是node之间的关系A一直都是共享的。  
假设构造一个2层的GCN，用ReLU和Softmax，整体的正向传播公式为：  
![GCN_forward](/users/anlei/desktop/笔记/pic/GCN_forward.png)
针对所有带标签的节点计算交叉熵损失函数：
![GCN_loss](/users/anlei/desktop/笔记/pic/GCN_loss.png)
就可以进行node分类了，即使只有很少的Node有标签也可以训练，所以是**半监督分类**。  
改变一下损失函数就可以去做图分类、边预测等任务。  
*  只使用A的话，由于A的对角线上都是0，所以在和特征矩阵H相乘的时候，只会计算一个node的所有邻居的特征的加权和，该node自己的特征却被忽略了。因此，我们可以做一个小小的改动，**给A加上一个单位矩阵I，这样就让对角线元素变成1了**。
*  **A是没有经过归一化的矩阵**，这样与特征矩阵相乘会改变特征原本的分布，产生一些不可预测的问题。所以我们对A做一个标准化处理。首先让A的每一行加起来为1，我们可以乘以一个D的逆，D就是度矩阵。我们可以进一步把D的拆开与A相乘，得到一个对称且归一化的矩阵 ：D^(-1/2)AD^(1/2)  
*  如果网络中没有节点的特征，可以使用单位矩阵I替换特征矩阵X作为GCN的输入
*  没有任何标注信息也可以用GCN来提取graph embedding
*  GCN层数不宜多，2-3层效果就很好了


