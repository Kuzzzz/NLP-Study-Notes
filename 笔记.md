#笔记
##Tensorflow
* 通过占位符导入数据，生成数据或设置数据管道。
* 通过计算图提供数据。
* 评估损失函数的输出。
* 使用反向传播来修改变量。
* 重复直到停止状态。  

##L1, L2
L1,L2正则化 可以看做是**损失函数的惩罚项**，也就是对损失函数中的某些参数做一些限制。  
L1正则化是指权值向量w中各个元素的绝对值之和。l1正则化可以产生稀疏权值矩阵，也就是一个稀疏模型，可用于特征选择  
L2正则化是指权值向量w中各个元素的平方和再求平方根。可以防止模型过拟合  
正则化项前一般需要加一个系数，需要用户指定  

*  作为损失函数，L2比L1用的多的原因是L2计算方便，因为在涉及绝对值时，求导很麻烦；而且用L2一定只有一条最好的预测线，而L1可能存在多个最优解。 

*  作为正则项，L2的计算更方便，L1在非稀疏向量上的计算效率很低；L1输出稀疏，会把不重要的特征直接置0，L2不会；L2有唯一解，L1不是。（在梯度更新时，无论L1的大小多少，只要不是0，梯度都是1或者-1，所以每次更新时都是一直向0靠近。而L2越靠近0梯度越小。所以经过一定步数后L1很可能变为0，而L2基本不可能，所以L1输出稀疏）  
*  L1正则化会使原最优解的元素产生不同量的偏移，并使某些元素为0，从而产生稀疏性（加强权值的稀疏性）
*  L2正则化对原最优解的每个元素进行不同比例的放缩（减小每次权重的调整幅度，避免训练过程中出现较大抖动）

##过拟合和欠拟合
###完全不收敛：
检查数据是否存在问题；网络的深度、非线性程度、分类器的种类等  
###部分收敛：
**欠拟合时**：增加网络的复杂度（深度）；降低学习率；优化数据集；增加网络的非线性程度（Relu）；采用Batch Normalization  
**过拟合时**：丰富数据；增加网络的稀疏度；降低网络的复杂度（深度）；L1、L2正则化；添加Dropout；Early stopping；降低学习率；减少epoch次数  
###全部收敛
一次只调整一个参数，需要调整的参数可能有：学习率、minibatch size、epoch、filter size、nums of filter  
###欠拟合解决策略
*  让模型更复杂（更深、更宽）
*  减少Normalization、减少dropout值、减少L2正则值
*  错误分析
*  选择性能更好的模型架构
*  调节超参数
*  加入更多特征

###过拟合解决策略
####Dropout
* Dropout在RNN/LSTM中主要作用在non-recurrent connections上，因为作用在recurrent connections上会丢失长期记忆信息。[rnn regularization]
*  **pass**  

##Normalization
为了把输入转化成均值为0、方差为1的数据，在激活函数前完成归一化，因为不希望输入数据落在激活函数的饱和区。
###Batch Normalization
在每一层的每一批数据上进行归一化，（input进行归一化后，经过网络层的作用后，数据已经不再是归一化的了。随着这种情况的发展，数据的偏差会越来越大，反向传播需要考虑这些大的偏差，只能用很小的学习率来防止梯度消失和爆炸）  
BN的具体做法是对每一小批数据，在batch这个方向上做归一化  
![BN](/users/anlei/desktop/笔记/pic/BN.png)
BN(x_{i})=\alpha \times \frac{x_{i}-\mu _{b}}{\sqrt{\sigma _{2}^{B}+\epsilon }}+\beta  
###Layer Normalization
在每一个样本上计算均值和方差，而不是像BN那也在批方向上计算。  
![LN](/users/anlei/desktop/笔记/pic/LN.png)  
##激活函数
激活函数的特性：  
非线性、几乎处处可微、计算简单、非饱和性、输出范围有限、接近恒等变换、参数少、归一化
###sigmoid
sigmoid主要把输出映射在[0,1]，\sigma(a)=\frac{1}{1+e^{-a}}  
单调连续，输出范围有限，优化稳定，可以用作输出层。但在两端存在饱和区域，随着网络深度的增加会导致梯度消失，使训练出现问题，同时输出不是以0为中心的。  
如果使用sigmoid，则NN隐层的每个节点输出都是正数，意味着同一层的权重梯度方向相同（都为正或负），
###tanh
\tanh (x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}=\frac{1-e^{-2 x}}{1+e^{-2 x}}  
相比于sigmoid，tanh的收敛速度快很多，输出以0为中心，缺点是依旧存在饱和区域，容易产生梯度消失。
###ReLU
y=max(0,x)  
#####优点
ReLU在SGD中能快速收敛，sigmoid、tanh的计算代价很大，ReLU实现简单；  
在正区域梯度恒定，没有饱和区域，从而缓解梯度消失；  
提供了NN的稀疏表达能力；  
#####缺点
随着训练的进行，可能出现神经元死亡，权重无法更新的情况（梯度永远为0）
** 假设有一个神经元，输入为x，经过线性层z=w*x，之后接ReLU, a=ReLU(z)，在训练中w的更新公式为：W \leftarrow w-l r * \frac{\partial L}{\partial w} 如果在某次更新中梯度太大，学习率也被设置的很大的话，权重就会更新过多，可能就会出现后续的任意样本输入，z都小于0，经过ReLU后输出a为0。在之后的反向传播过程中，a为0，L对w求偏导也为0，权重矩阵W不再更新，神经元相当于死亡了。
###GELU
##调参技巧
模型错误率较高的原因可能有：  

*  模型实现中的BUG:如标签错误的问题
*  超参数选择不合适:模型对超参数很敏感，lr太高太低都不行
*  数据和模型不匹配:如用自动驾驶图像识别模型+imagenet数据集
*  数据集的构造问题：数据不够、分类不均匀、有噪声标签、训练集和测试集的数据分布不同等  
##事件抽取
* 事件识别和抽取
* 事件检测和追踪

